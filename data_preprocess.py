import os
import json
import torch
import numpy as np
from typing import Dict, List, Any
from data_process.preprocess import load_dataset, create_observations
from data_process.cloud import fit_sphere_center_TLS_A


def save_observations_to_json(observations: List[Dict[str, Any]], output_file: str) -> bool:
    """
    å°†è§‚æµ‹æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶
    
    Args:
        observations: è§‚æµ‹æ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        # è½¬æ¢æ•°æ®æ ¼å¼ä»¥ä¾¿JSONåºåˆ—åŒ–
        json_data = {
            "observations": []
        }
        
        for i, obs in enumerate(observations):
            # è®¡ç®—ç‚¹äº‘ä¸­å¿ƒ
            point_cloud_center = None
            if obs["point_cloud"] is not None:
                try:
                    # è½¬æ¢ä¸ºtorchå¼ é‡
                    points_tensor = torch.tensor(obs["point_cloud"], dtype=torch.float64)
                    # ä½¿ç”¨çƒå¿ƒæ‹Ÿåˆç®—æ³•è®¡ç®—ä¸­å¿ƒ
                    center_tensor = fit_sphere_center_TLS_A(points_tensor)
                    point_cloud_center = center_tensor.detach().cpu().numpy().tolist()
                except Exception as e:
                    print(f"âš ï¸  è§‚æµ‹ {i} ç‚¹äº‘ä¸­å¿ƒè®¡ç®—å¤±è´¥: {str(e)}")
                    point_cloud_center = None
            
            obs_data = {
                "id": i,
                "joint_state": obs["joint_state"].tolist() if obs["joint_state"] is not None else None,
                "point_cloud_center": point_cloud_center  # ä¿å­˜ç‚¹äº‘ä¸­å¿ƒåæ ‡è€Œä¸æ˜¯å®Œæ•´ç‚¹äº‘
            }
            json_data["observations"].append(obs_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ æˆåŠŸä¿å­˜ {len(observations)} ä¸ªè§‚æµ‹æ•°æ®åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜è§‚æµ‹æ•°æ®å¤±è´¥: {str(e)}")
        return False


def process_dataset_folder(input_folder: str, output_folder: str, force_overwrite: bool = False) -> bool:
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†æ–‡ä»¶å¤¹
    
    Args:
        input_folder: è¾“å…¥æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        output_folder: è¾“å‡ºæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        æ˜¯å¦å¤„ç†æˆåŠŸ
    """
    try:
        print(f"\n=== å¤„ç†æ•°æ®é›†: {input_folder} ===")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        output_file = os.path.join(output_folder, "observations.json")
        if os.path.exists(output_file) and not force_overwrite:
            print(f"â­ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {output_file}")
            return True
        elif os.path.exists(output_file) and force_overwrite:
            print(f"ğŸ”„ è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œä½†å¼ºåˆ¶é‡æ–°å¤„ç†: {output_file}")
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_folder):
            print(f"âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
            return False
        
        # æ£€æŸ¥poses.jsonæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        poses_file = os.path.join(input_folder, "poses.json")
        if not os.path.exists(poses_file):
            print(f"âŒ ä½å§¿æ–‡ä»¶ä¸å­˜åœ¨: {poses_file}")
            return False
        
        # æ£€æŸ¥cloudsæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        clouds_folder = os.path.join(input_folder, "clouds")
        if not os.path.exists(clouds_folder):
            print(f"âŒ ç‚¹äº‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {clouds_folder}")
            return False
        
        # è®¡ç®—ç‚¹äº‘æ–‡ä»¶æ•°é‡
        ply_files = [f for f in os.listdir(clouds_folder) if f.endswith('.ply')]
        num_clouds = len(ply_files)
        print(f"å‘ç° {num_clouds} ä¸ªç‚¹äº‘æ–‡ä»¶")
        
        if num_clouds == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‚¹äº‘æ–‡ä»¶")
            return False
        
        # åŠ è½½æ•°æ®é›†
        poses, point_clouds = load_dataset(
            data_dir=input_folder,
            num_samples=num_clouds,
            poses_file="poses.json",
            clouds_subdir="clouds",
            convert_to_rad=True
        )
        
        if poses is None or point_clouds is None:
            print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
            return False
        
        # åˆ›å»ºè§‚æµ‹æ•°æ®
        observations = create_observations(poses, point_clouds)
        
        if len(observations) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§‚æµ‹æ•°æ®")
            return False
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(output_folder, "observations.json")
        
        # ä¿å­˜è§‚æµ‹æ•°æ®
        success = save_observations_to_json(observations, output_file)
        
        if success:
            print(f"âœ“ æ•°æ®é›†å¤„ç†å®Œæˆ: {len(observations)} ä¸ªè§‚æµ‹æ•°æ®")
            return True
        else:
            return False
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®é›†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def preprocess_all_datasets(origin_data_dir: str = None, output_base_dir: str = None, force_overwrite: bool = False) -> Dict[str, bool]:
    """
    æ•°æ®é¢„å¤„ç†ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†
    
    Args:
        origin_data_dir: åŸå§‹æ•°æ®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„origin_data
        output_base_dir: è¾“å‡ºæ•°æ®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„data
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸ï¼Œé”®ä¸ºæ•°æ®é›†è·¯å¾„ï¼Œå€¼ä¸ºæ˜¯å¦å¤„ç†æˆåŠŸ
    """
    # è®¾ç½®é»˜è®¤è·¯å¾„
    if origin_data_dir is None:
        origin_data_dir = os.path.join(os.getcwd(), "origin_data")
    
    if output_base_dir is None:
        output_base_dir = os.path.join(os.getcwd(), "data")
    
    print("=== æ•°æ®é¢„å¤„ç†ä¸»å‡½æ•° ===")
    print(f"åŸå§‹æ•°æ®ç›®å½•: {origin_data_dir}")
    print(f"è¾“å‡ºæ•°æ®ç›®å½•: {output_base_dir}")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(origin_data_dir):
        print(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {origin_data_dir}")
        return {}
    
    # å®šä¹‰è¦å¤„ç†çš„æ•°æ®é›†è·¯å¾„æ˜ å°„
    dataset_mapping = {
        # æ‰‹çœ¼æ ‡å®šæ•°æ®
        "hand_eye_data/pose0_test1": "hand_eye_data/pose0_test1",
        "hand_eye_data/pose0_test2": "hand_eye_data/pose0_test2",
        "hand_eye_data/pose2_test1": "hand_eye_data/pose2_test1",
        
        # ä¸»è¦æ ‡å®šæ•°æ®
        "main_data/pose0": "main_data/pose0",
        "main_data/pose2": "main_data/pose2",
        "main_data/pose3": "main_data/pose3",
        "main_data/pose4": "main_data/pose4",
        
        # æµ‹è¯•æ•°æ®
        "test_data/pose0": "test_data/pose0",
        "test_data/pose1": "test_data/pose1"
    }
    
    # å¤„ç†ç»“æœ
    results = {}
    successful_count = 0
    total_count = len(dataset_mapping)
    
    # æ‰¹é‡å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for input_path, output_path in dataset_mapping.items():
        input_folder = os.path.join(origin_data_dir, input_path)
        output_folder = os.path.join(output_base_dir, output_path)
        
        # å¤„ç†æ•°æ®é›†
        success = process_dataset_folder(input_folder, output_folder, force_overwrite)
        results[input_path] = success
        
        if success:
            successful_count += 1
    
    # æ‰“å°å¤„ç†æ€»ç»“
    print(f"\n=== æ•°æ®é¢„å¤„ç†å®Œæˆ ===")
    print(f"æ€»æ•°æ®é›†æ•°é‡: {total_count}")
    print(f"æˆåŠŸå¤„ç†: {successful_count}")
    print(f"å¤±è´¥æ•°é‡: {total_count - successful_count}")
    
    # è¯¦ç»†ç»“æœ
    print(f"\n=== è¯¦ç»†å¤„ç†ç»“æœ ===")
    for dataset_path, success in results.items():
        status = "âœ“ æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{status}: {dataset_path}")
    
    return results


def preprocess_single_dataset(dataset_path: str, output_path: str = None, force_overwrite: bool = False) -> bool:
    """
    å¤„ç†å•ä¸ªæŒ‡å®šçš„æ•°æ®é›†
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆç›¸å¯¹äºorigin_dataæˆ–ç»å¯¹è·¯å¾„ï¼‰
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        æ˜¯å¦å¤„ç†æˆåŠŸ
    """
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™æ·»åŠ origin_dataå‰ç¼€
    if not os.path.isabs(dataset_path):
        origin_data_dir = os.path.join(os.getcwd(), "origin_data")
        input_folder = os.path.join(origin_data_dir, dataset_path)
    else:
        input_folder = dataset_path
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
    if output_path is None:
        data_dir = os.path.join(os.getcwd(), "data")
        if not os.path.isabs(dataset_path):
            output_folder = os.path.join(data_dir, dataset_path)
        else:
            # ä»ç»å¯¹è·¯å¾„æå–ç›¸å¯¹éƒ¨åˆ†
            rel_path = os.path.relpath(dataset_path, os.path.join(os.getcwd(), "origin_data"))
            output_folder = os.path.join(data_dir, rel_path)
    else:
        output_folder = output_path
    
    print(f"å¤„ç†å•ä¸ªæ•°æ®é›†: {input_folder} -> {output_folder}")
    return process_dataset_folder(input_folder, output_folder, force_overwrite)


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # ç¤ºä¾‹1: å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
    print("=== ç¤ºä¾‹1: æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆè·³è¿‡å·²å­˜åœ¨ï¼‰ ===")
    results = preprocess_all_datasets()
    print("å¤„ç†ç»“æœ:", results)