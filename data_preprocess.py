import os
import json
import torch
import numpy as np
import re
from typing import Dict, List, Any, Tuple
from data_process.preprocess import load_dataset, create_observations
from data_process.cloud import fit_sphere_center_TLS_A


def detect_poses_file_format(data_dir: str) -> Tuple[str, str]:
    """
    è‡ªåŠ¨æ£€æµ‹ä½å§¿æ–‡ä»¶æ ¼å¼
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        (file_format, file_path) å…ƒç»„
        file_format: "txt", "json", æˆ– "none"
        file_path: æ£€æµ‹åˆ°çš„æ–‡ä»¶è·¯å¾„
    """
    # æ£€æŸ¥txtæ ¼å¼æ–‡ä»¶
    poses_txt_path = os.path.join(data_dir, "poses.txt")
    if os.path.exists(poses_txt_path):
        return "txt", poses_txt_path
    
    # æ£€æŸ¥jsonæ ¼å¼æ–‡ä»¶
    poses_json_path = os.path.join(data_dir, "poses.json")
    if os.path.exists(poses_json_path):
        return "json", poses_json_path
    
    return "none", ""


def load_dataset_auto_format(data_dir: str, num_samples: int = None, 
                           clouds_subdir: str = "clouds") -> Tuple[torch.Tensor, List]:
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼å¹¶åŠ è½½æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        num_samples: è¦åŠ è½½çš„æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
        clouds_subdir: ç‚¹äº‘æ–‡ä»¶å­ç›®å½•å
        
    Returns:
        (poses, point_clouds) å…ƒç»„
        poses: torch.Tensorï¼Œå½¢çŠ¶ä¸º (N, 6)
        point_clouds: ç‚¹äº‘æ•°æ®åˆ—è¡¨
    """
    try:
        # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼
        file_format, file_path = detect_poses_file_format(data_dir)
        
        if file_format == "none":
            print(f"âŒ åœ¨ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°poses.txtæˆ–poses.jsonæ–‡ä»¶")
            return None, None
        
        print(f"âœ“ æ£€æµ‹åˆ°ä½å§¿æ–‡ä»¶æ ¼å¼: {file_format} ({file_path})")
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©åŠ è½½æ–¹æ³•
        if file_format == "txt":
            return load_dataset_from_txt(
                data_dir=data_dir,
                num_samples=num_samples,
                poses_file="poses.txt",
                clouds_subdir=clouds_subdir
            )
        elif file_format == "json":
            return load_dataset(
                data_dir=data_dir,
                num_samples=num_samples,
                poses_file="poses.json",
                clouds_subdir=clouds_subdir,
                convert_to_rad=True
            )
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
            return None, None
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨æ ¼å¼åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


def parse_joint_states_from_txt(txt_file_path: str) -> List[np.ndarray]:
    """
    ä»txtæ–‡ä»¶è§£æjoint_statesæ•°æ®
    
    Args:
        txt_file_path: txtæ–‡ä»¶è·¯å¾„
        
    Returns:
        joint_statesåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º6ç»´numpyæ•°ç»„
    """
    joint_states = []
    
    try:
        with open(txt_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–joint_states_positionæ•°æ®
        pattern = r"joint_states_position:\s*array\('d',\s*\[(.*?)\]\)"
        matches = re.findall(pattern, content)
        
        for match in matches:
            # åˆ†å‰²æ•°å­—å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            numbers_str = match.strip()
            numbers = [float(x.strip()) for x in numbers_str.split(',') if x.strip()]
            
            if len(numbers) == 6:  # ç¡®ä¿æ˜¯6ä¸ªå…³èŠ‚
                joint_states.append(np.array(numbers))
            else:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆçš„å…³èŠ‚çŠ¶æ€æ•°æ®ï¼ˆå…³èŠ‚æ•°ä¸ä¸º6ï¼‰: {numbers}")
        
        print(f"âœ“ ä» {txt_file_path} æˆåŠŸè§£æ {len(joint_states)} ä¸ªå…³èŠ‚çŠ¶æ€")
        return joint_states
        
    except Exception as e:
        print(f"âŒ è§£ætxtæ–‡ä»¶å¤±è´¥ {txt_file_path}: {str(e)}")
        return []


def load_dataset_from_txt(data_dir: str, num_samples: int = None, 
                         poses_file: str = "poses.txt", 
                         clouds_subdir: str = "clouds") -> Tuple[torch.Tensor, List]:
    """
    ä»txtæ ¼å¼æ–‡ä»¶åŠ è½½æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        num_samples: è¦åŠ è½½çš„æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
        poses_file: ä½å§¿æ–‡ä»¶åï¼ˆtxtæ ¼å¼ï¼‰
        clouds_subdir: ç‚¹äº‘æ–‡ä»¶å­ç›®å½•å
        
    Returns:
        (poses, point_clouds) å…ƒç»„
        poses: torch.Tensorï¼Œå½¢çŠ¶ä¸º (N, 6)
        point_clouds: ç‚¹äº‘æ•°æ®åˆ—è¡¨
    """
    try:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        poses_path = os.path.join(data_dir, poses_file)
        clouds_path = os.path.join(data_dir, clouds_subdir)
        
        # æ£€æŸ¥æ–‡ä»¶å’Œç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(poses_path):
            print(f"âŒ ä½å§¿æ–‡ä»¶ä¸å­˜åœ¨: {poses_path}")
            return None, None
            
        if not os.path.exists(clouds_path):
            print(f"âŒ ç‚¹äº‘ç›®å½•ä¸å­˜åœ¨: {clouds_path}")
            return None, None
        
        # è§£æjoint_statesæ•°æ®
        joint_states_list = parse_joint_states_from_txt(poses_path)
        
        if len(joint_states_list) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å…³èŠ‚çŠ¶æ€æ•°æ®")
            return None, None
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if num_samples is not None:
            joint_states_list = joint_states_list[:num_samples]
        
        # è½¬æ¢ä¸ºtorchå¼ é‡
        poses = torch.tensor(np.array(joint_states_list), dtype=torch.float64)
        
        # åŠ è½½ç‚¹äº‘æ•°æ®ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
        from data_process.preprocess import load_dataset
        _, point_clouds = load_dataset(
            data_dir=data_dir,
            num_samples=len(joint_states_list),
            clouds_subdir=clouds_subdir,
            convert_to_rad=False  # txtæ–‡ä»¶ä¸­çš„æ•°æ®å·²ç»æ˜¯å¼§åº¦
        )
        
        print(f"âœ“ æˆåŠŸä»txtæ ¼å¼åŠ è½½æ•°æ®é›†: {len(joint_states_list)} ä¸ªä½å§¿ï¼Œ{len(point_clouds) if point_clouds else 0} ä¸ªç‚¹äº‘")
        return poses, point_clouds
        
    except Exception as e:
        print(f"âŒ ä»txtæ ¼å¼åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None


def save_observations_to_json(observations: List[Dict[str, Any]], output_file: str) -> bool:
    """
    å°†è§‚æµ‹æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶
    
    Args:
        observations: è§‚æµ‹æ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        # è½¬æ¢æ•°æ®æ ¼å¼ä»¥ä¾¿JSONåºåˆ—åŒ–
        json_data = {
            "observations": []
        }
        
        for i, obs in enumerate(observations):
            # è®¡ç®—ç‚¹äº‘ä¸­å¿ƒ
            point_cloud_center = None
            if obs["point_cloud"] is not None:
                try:
                    # è½¬æ¢ä¸ºtorchå¼ é‡
                    points_tensor = torch.tensor(obs["point_cloud"], dtype=torch.float64)
                    # ä½¿ç”¨çƒå¿ƒæ‹Ÿåˆç®—æ³•è®¡ç®—ä¸­å¿ƒ
                    center_tensor = fit_sphere_center_TLS_A(points_tensor)
                    point_cloud_center = center_tensor.detach().cpu().numpy().tolist()
                except Exception as e:
                    print(f"âš ï¸  è§‚æµ‹ {i} ç‚¹äº‘ä¸­å¿ƒè®¡ç®—å¤±è´¥: {str(e)}")
                    point_cloud_center = None
            
            obs_data = {
                "id": i,
                "joint_state": obs["joint_state"].tolist() if obs["joint_state"] is not None else None,
                "point_cloud_center": point_cloud_center  # ä¿å­˜ç‚¹äº‘ä¸­å¿ƒåæ ‡è€Œä¸æ˜¯å®Œæ•´ç‚¹äº‘
            }
            json_data["observations"].append(obs_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ“ æˆåŠŸä¿å­˜ {len(observations)} ä¸ªè§‚æµ‹æ•°æ®åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜è§‚æµ‹æ•°æ®å¤±è´¥: {str(e)}")
        return False


def process_dataset_folder(input_folder: str, output_folder: str, force_overwrite: bool = False) -> bool:
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†æ–‡ä»¶å¤¹
    
    Args:
        input_folder: è¾“å…¥æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        output_folder: è¾“å‡ºæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        æ˜¯å¦å¤„ç†æˆåŠŸ
    """
    try:
        print(f"\n=== å¤„ç†æ•°æ®é›†: {input_folder} ===")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        output_file = os.path.join(output_folder, "observations.json")
        if os.path.exists(output_file) and not force_overwrite:
            print(f"â­ï¸  è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {output_file}")
            return True
        elif os.path.exists(output_file) and force_overwrite:
            print(f"ğŸ”„ è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œä½†å¼ºåˆ¶é‡æ–°å¤„ç†: {output_file}")
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_folder):
            print(f"âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
            return False
        
        # è‡ªåŠ¨æ£€æµ‹ä½å§¿æ–‡ä»¶æ ¼å¼
        file_format, file_path = detect_poses_file_format(input_folder)
        
        if file_format == "none":
            print(f"âŒ åœ¨ {input_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°poses.txtæˆ–poses.jsonæ–‡ä»¶")
            return False
        
        print(f"âœ“ æ£€æµ‹åˆ°ä½å§¿æ–‡ä»¶æ ¼å¼: {file_format} ({os.path.basename(file_path)})")
        
        # æ£€æŸ¥cloudsæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        clouds_folder = os.path.join(input_folder, "clouds")
        if not os.path.exists(clouds_folder):
            print(f"âŒ ç‚¹äº‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {clouds_folder}")
            return False
        
        # è®¡ç®—ç‚¹äº‘æ–‡ä»¶æ•°é‡
        ply_files = [f for f in os.listdir(clouds_folder) if f.endswith('.ply')]
        num_clouds = len(ply_files)
        print(f"å‘ç° {num_clouds} ä¸ªç‚¹äº‘æ–‡ä»¶")
        
        if num_clouds == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç‚¹äº‘æ–‡ä»¶")
            return False
        
        # ä½¿ç”¨è‡ªåŠ¨æ ¼å¼æ£€æµ‹åŠ è½½æ•°æ®é›†
        poses, point_clouds = load_dataset_auto_format(
            data_dir=input_folder,
            num_samples=num_clouds,
            clouds_subdir="clouds"
        )
        
        if poses is None or point_clouds is None:
            print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
            return False
        
        # åˆ›å»ºè§‚æµ‹æ•°æ®
        observations = create_observations(poses, point_clouds)
        
        if len(observations) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§‚æµ‹æ•°æ®")
            return False
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(output_folder, "observations.json")
        
        # ä¿å­˜è§‚æµ‹æ•°æ®
        success = save_observations_to_json(observations, output_file)
        
        if success:
            print(f"âœ“ æ•°æ®é›†å¤„ç†å®Œæˆ ({file_format}æ ¼å¼): {len(observations)} ä¸ªè§‚æµ‹æ•°æ®")
            return True
        else:
            return False
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®é›†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def preprocess_all_datasets(origin_data_dir: str = None, output_base_dir: str = None, force_overwrite: bool = False) -> Dict[str, bool]:
    """
    æ•°æ®é¢„å¤„ç†ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†
    
    Args:
        origin_data_dir: åŸå§‹æ•°æ®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„origin_data
        output_base_dir: è¾“å‡ºæ•°æ®æ ¹ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„data
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸ï¼Œé”®ä¸ºæ•°æ®é›†è·¯å¾„ï¼Œå€¼ä¸ºæ˜¯å¦å¤„ç†æˆåŠŸ
    """
    # è®¾ç½®é»˜è®¤è·¯å¾„
    if origin_data_dir is None:
        origin_data_dir = os.path.join(os.getcwd(), "origin_data")
    
    if output_base_dir is None:
        output_base_dir = os.path.join(os.getcwd(), "data")
    
    print("=== æ•°æ®é¢„å¤„ç†ä¸»å‡½æ•° ===")
    print(f"åŸå§‹æ•°æ®ç›®å½•: {origin_data_dir}")
    print(f"è¾“å‡ºæ•°æ®ç›®å½•: {output_base_dir}")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(origin_data_dir):
        print(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {origin_data_dir}")
        return {}
    
    # å®šä¹‰è¦å¤„ç†çš„æ•°æ®é›†è·¯å¾„æ˜ å°„
    dataset_mapping = {
        # æ‰‹çœ¼æ ‡å®šæ•°æ®
        "hand_eye_data/pose0_test1": "hand_eye_data/pose0_test1",
        "hand_eye_data/pose0_test2": "hand_eye_data/pose0_test2",
        "hand_eye_data/pose2_test1": "hand_eye_data/pose2_test1",
        "hand_eye_data/ABB": "hand_eye_data/ABB",
        "hand_eye_data/ABB_2": "hand_eye_data/ABB_2",
        "hand_eye_data/ABB_3": "hand_eye_data/ABB_3",
        "hand_eye_data/ABB_4": "hand_eye_data/ABB_4",
        "hand_eye_data/ABB_5": "hand_eye_data/ABB_5",
        
        
        # ä¸»è¦æ ‡å®šæ•°æ®
        "main_data/pose0": "main_data/pose0",
        "main_data/pose2": "main_data/pose2",
        "main_data/pose3": "main_data/pose3",
        "main_data/pose4": "main_data/pose4",
        "main_data/ABB_1": "main_data/ABB_1",
        "main_data/ABB_2": "main_data/ABB_2",
        
        # æµ‹è¯•æ•°æ®
        "test_data/pose0": "test_data/pose0",
        "test_data/pose1": "test_data/pose1"
    }
    
    # å¤„ç†ç»“æœ
    results = {}
    successful_count = 0
    total_count = len(dataset_mapping)
    
    # æ‰¹é‡å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for input_path, output_path in dataset_mapping.items():
        input_folder = os.path.join(origin_data_dir, input_path)
        output_folder = os.path.join(output_base_dir, output_path)
        
        # å¤„ç†æ•°æ®é›†
        success = process_dataset_folder(input_folder, output_folder, force_overwrite)
        results[input_path] = success
        
        if success:
            successful_count += 1
    
    # æ‰“å°å¤„ç†æ€»ç»“
    print(f"\n=== æ•°æ®é¢„å¤„ç†å®Œæˆ ===")
    print(f"æ€»æ•°æ®é›†æ•°é‡: {total_count}")
    print(f"æˆåŠŸå¤„ç†: {successful_count}")
    print(f"å¤±è´¥æ•°é‡: {total_count - successful_count}")
    
    # è¯¦ç»†ç»“æœ
    print(f"\n=== è¯¦ç»†å¤„ç†ç»“æœ ===")
    for dataset_path, success in results.items():
        status = "âœ“ æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{status}: {dataset_path}")
    
    return results


def preprocess_single_dataset(dataset_path: str, output_path: str = None, force_overwrite: bool = False) -> bool:
    """
    å¤„ç†å•ä¸ªæŒ‡å®šçš„æ•°æ®é›†
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„ï¼ˆç›¸å¯¹äºorigin_dataæˆ–ç»å¯¹è·¯å¾„ï¼‰
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        force_overwrite: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶
        
    Returns:
        æ˜¯å¦å¤„ç†æˆåŠŸ
    """
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™æ·»åŠ origin_dataå‰ç¼€
    if not os.path.isabs(dataset_path):
        origin_data_dir = os.path.join(os.getcwd(), "origin_data")
        input_folder = os.path.join(origin_data_dir, dataset_path)
    else:
        input_folder = dataset_path
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„
    if output_path is None:
        data_dir = os.path.join(os.getcwd(), "data")
        if not os.path.isabs(dataset_path):
            output_folder = os.path.join(data_dir, dataset_path)
        else:
            # ä»ç»å¯¹è·¯å¾„æå–ç›¸å¯¹éƒ¨åˆ†
            rel_path = os.path.relpath(dataset_path, os.path.join(os.getcwd(), "origin_data"))
            output_folder = os.path.join(data_dir, rel_path)
    else:
        output_folder = output_path
    
    print(f"å¤„ç†å•ä¸ªæ•°æ®é›†: {input_folder} -> {output_folder}")
    return process_dataset_folder(input_folder, output_folder, force_overwrite)


# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # ç¤ºä¾‹1: å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
    print("=== ç¤ºä¾‹1: æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆè·³è¿‡å·²å­˜åœ¨ï¼‰ ===")
    results = preprocess_all_datasets()
    print("å¤„ç†ç»“æœ:", results)